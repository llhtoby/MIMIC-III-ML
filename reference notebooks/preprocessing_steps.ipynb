{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "senior-video",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "In the previous script (extract_patients.ipynb), 17 clinical variables were extracted. Here we will do the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "herbal-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-camera",
   "metadata": {},
   "source": [
    "### Read extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_data (X):  (20567026, 14)\n",
      "patients_data:  (30063, 25)\n",
      "outcomes (Y):  (30063, 2)\n",
      "Categorical:  ['Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', 'Glascow coma scale verbal response']\n",
      "Continuous:  ['Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n"
     ]
    }
   ],
   "source": [
    "events_data = pd.read_hdf('vitals_hourly_data.h5', 'X')\n",
    "events_data = events_data.reset_index()\n",
    "print('events_data (X): ', events_data.shape)\n",
    "\n",
    "patients_data = pd.read_hdf('vitals_hourly_data.h5', 'patients_data')\n",
    "print('patients_data: ', patients_data.shape)\n",
    "\n",
    "outcomes = pd.read_hdf('vitals_hourly_data.h5', 'Y')\n",
    "print('outcomes (Y): ', outcomes.shape)\n",
    "\n",
    "# Load the config file that contains information about continuous/categorical variables:\n",
    "config = json.load(open('E:/MIMIC-III-ML/Preprocessing/resources/discretizer_config.json', 'r'))\n",
    "is_categorical = config['is_categorical_channel']\n",
    "\n",
    "# Get categorical variables:\n",
    "categorical_var = []\n",
    "continuous_var = []\n",
    "for key, value in is_categorical.items():\n",
    "    if value:\n",
    "        categorical_var.append(key)\n",
    "    else:\n",
    "        continuous_var.append(key)\n",
    "print('Categorical: ', categorical_var[1:])\n",
    "print('Continuous: ', continuous_var)\n",
    "categorical_var = categorical_var[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-illustration",
   "metadata": {},
   "source": [
    "### Pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electronic-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map variables to the same metric:\n",
    "UNIT_CONVERSIONS = [\n",
    "    ('weight',                   'oz',  None,             lambda x: x/16.*0.45359237),\n",
    "    ('weight',                   'lbs', None,             lambda x: x*0.45359237),\n",
    "    ('fraction inspired oxygen', None,  lambda x: x > 1,  lambda x: x/100.),\n",
    "    ('oxygen saturation',        None,  lambda x: x <= 1, lambda x: x*100.),\n",
    "    ('temperature',              'f',   lambda x: x > 79, lambda x: (x - 32) * 5./9),\n",
    "    ('height',                   'in',  None,             lambda x: x*2.54),\n",
    "]\n",
    "\n",
    "variable_names = events_data['LEVEL1'].str\n",
    "variable_units = events_data['valueuom'].str\n",
    "for name, unit, check, convert_function in UNIT_CONVERSIONS:\n",
    "    indices_variable = variable_names.contains(name, case=False, na=False)\n",
    "    needs_conversion_filter_indices = indices_variable & False\n",
    "    if unit is not None:\n",
    "        needs_conversion_filter_indices |= variable_names.contains(unit, case=False, na=False) | variable_units.contains(unit, case=False, na=False)\n",
    "    if check is not None:\n",
    "        needs_conversion_filter_indices |= check(events_data['value'])\n",
    "    idx = indices_variable & needs_conversion_filter_indices\n",
    "    events_data.loc[idx, 'value'] = convert_function(events_data['value'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impossible-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove outliers. For this, they use two different outlier ranges: \n",
    "# 1) for each variable, they have an upper and lower threshold for detecting unusable outliers. \n",
    "#    If the outlier falls outside of these threshold, it is treated as missing. \n",
    "# 2) they also have a physiologically valid range of measurements. If the non-outlier falls outside this range, \n",
    "     # it is replaced with the nearest valid value.\n",
    "\n",
    "variable_ranges = pd.read_csv('E:/MIMIC-III-ML/Preprocessing/resources/variable_ranges.csv', index_col = None)\n",
    "variable_ranges['LEVEL2'] = variable_ranges['LEVEL2'].str.lower()\n",
    "variable_ranges = variable_ranges.set_index('LEVEL2')\n",
    "\n",
    "variables_all = events_data['LEVEL2']\n",
    "non_null_variables = ~events_data.value.isnull()\n",
    "variables = set(variables_all)\n",
    "range_names = set(variable_ranges.index.values)\n",
    "range_names = [i.lower() for i in range_names]\n",
    "\n",
    "for var_name in variables:\n",
    "    var_name_lower = var_name.lower()\n",
    "    \n",
    "    if var_name_lower in range_names:\n",
    "        out_low, out_high, val_low, val_high = [\n",
    "            variable_ranges.loc[var_name_lower, x] for x in ('OUTLIER LOW', 'OUTLIER HIGH', 'VALID LOW', 'VALID HIGH')\n",
    "        ]\n",
    "        \n",
    "        # First find the indices of the variables that we need to check for outliers:\n",
    "        indices_variable = non_null_variables & (variables_all == var_name)\n",
    "        \n",
    "        # Check for low outliers and if they are not extreme, replace them with the imputation value:\n",
    "        outlier_low_indices = (events_data.value < out_low)\n",
    "        low_not_outliers = ~outlier_low_indices & (events_data.value < val_low)\n",
    "        valid_low_indices = indices_variable & low_not_outliers\n",
    "        events_data.loc[valid_low_indices, 'value'] = val_low\n",
    "        \n",
    "        # Check for high outliers and if they are not extreme, replace them with the imputation value:\n",
    "        outlier_high_indices = (events_data.value > out_high)\n",
    "        high_not_outliers = ~outlier_high_indices & (events_data.value > val_high)\n",
    "        valid_high_indices = indices_variable & high_not_outliers\n",
    "        events_data.loc[valid_high_indices, 'value'] = val_high\n",
    "        \n",
    "        # Treat values that are outside the outlier boundaries as missing:\n",
    "        outlier_indices = indices_variable & (outlier_low_indices | outlier_high_indices)\n",
    "        events_data.loc[outlier_indices, 'value'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-democrat",
   "metadata": {},
   "source": [
    "### Reshape data\n",
    "We want to have a column for every variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data = events_data.set_index(['icustay_id', 'itemid', 'label', 'LEVEL1', 'LEVEL2'])\n",
    "events_data = events_data.groupby(['icustay_id', 'subject_id', 'hadm_id', 'LEVEL2', 'hours_in'])\n",
    "events_data = events_data.agg(['mean', 'std', 'count'])\n",
    "events_data.columns = events_data.columns.droplevel(0)\n",
    "events_data.columns.names = ['Aggregation Function']\n",
    "events_data = events_data.unstack(level = 'LEVEL2')\n",
    "events_data.columns = events_data.columns.reorder_levels(order=['LEVEL2', 'Aggregation Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abroad-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a row for every hour:\n",
    "missing_hours_fill = pd.DataFrame([[i, x] for i, y in patients_data['max_hours'].iteritems() for x in range(y+1)],\n",
    "                                 columns=[patients_data.index.names[0], 'hours_in'])\n",
    "missing_hours_fill['tmp'] = np.NaN\n",
    "\n",
    "fill_df = patients_data.reset_index()[['subject_id', 'hadm_id', 'icustay_id']].join(\n",
    "     missing_hours_fill.set_index('icustay_id'), on='icustay_id')\n",
    "fill_df.set_index(['icustay_id', 'subject_id', 'hadm_id', 'hours_in'], inplace=True)\n",
    "\n",
    "events_data = events_data.reindex(fill_df.index)\n",
    "events_data = events_data.sort_index(axis = 0).sort_index(axis = 1)\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "events_data.loc[:, idx[:, 'count']] = events_data.loc[:, idx[:, 'count']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corrected-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this version of the data as a .csv file, so we can apply different imputation methods in another notebook:\n",
    "idx = pd.IndexSlice\n",
    "timeseries_data = events_data.loc[:, idx[:, 'mean']]\n",
    "timeseries_data = timeseries_data.droplevel('Aggregation Function', axis = 1) \n",
    "timeseries_data = timeseries_data.reset_index() \n",
    "timeseries_data.to_csv('mimic_timeseries_data_not_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-chicken",
   "metadata": {},
   "source": [
    "### Imputation of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "international-liberty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "timeseries_data = events_data.loc[:, idx[:, ['mean', 'count']]]\n",
    "\n",
    "# Get the mean across hours for each variable and each patient:\n",
    "icustay_means = timeseries_data.loc[:, idx[:, 'mean']].groupby(['subject_id', 'hadm_id', 'icustay_id']).mean()\n",
    "\n",
    "# Get the global mean for each variable:\n",
    "global_means = timeseries_data.loc[:, idx[:, 'mean']].mean(axis = 0)\n",
    "\n",
    "# Forward fill the nan time series, or otherwise fill in the patient's mean or global mean:\n",
    "timeseries_data.loc[:, idx[:, 'mean']] = timeseries_data.loc[:, idx[:, 'mean']].groupby(\n",
    "    ['subject_id', 'hadm_id', 'icustay_id']).fillna(method='ffill').groupby(\n",
    "    ['subject_id', 'hadm_id', 'icustay_id']).fillna(icustay_means).fillna(global_means)\n",
    "\n",
    "# Create a mask that indicates if the variable is present:\n",
    "timeseries_data.loc[:, idx[:, 'count']] = (events_data.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "timeseries_data.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "# Add a variable that indicates the time since the last measurement to the dataframe:\n",
    "is_absent = (1 - timeseries_data.loc[:, idx[:, 'mask']])\n",
    "hours_of_absence = is_absent.cumsum()\n",
    "time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "timeseries_data = pd.concat((timeseries_data, time_since_measured), axis = 1)\n",
    "timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "timeseries_data.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-challenge",
   "metadata": {},
   "source": [
    "### Standardization of continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "allied-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minmax standardization:\n",
    "def minmax(x):\n",
    "    mins = x.min()\n",
    "    maxes = x.max()\n",
    "    x_std = (x - mins) / (maxes - mins)\n",
    "    return x_std\n",
    "\n",
    "def std_time_since_measurement(x):\n",
    "    idx = pd.IndexSlice\n",
    "    x = np.where(x==100, 0, x)\n",
    "    means = x.mean()\n",
    "    stds = x.std() + 0.0001\n",
    "    x_std = (x - means)/stds\n",
    "    return x_std\n",
    "\n",
    "timeseries_data.loc[:, idx[continuous_var, 'mean']] = timeseries_data.loc[:, idx[continuous_var, 'mean']].apply(lambda x: minmax(x))\n",
    "timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-swimming",
   "metadata": {},
   "source": [
    "### One-hot encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "agricultural-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to round the categorical variables to the nearest category:\n",
    "categorical_data = timeseries_data.loc[:, idx[categorical_var, 'mean']].copy(deep=True)\n",
    "categorical_data = categorical_data.round()\n",
    "one_hot = pd.get_dummies(categorical_data, columns=categorical_var)\n",
    "\n",
    "# Clean up the columns that we do not need and add the dummy encodings:\n",
    "for c in categorical_var:\n",
    "    if c in timeseries_data.columns:\n",
    "        timeseries_data.drop(c, axis = 1, inplace=True)\n",
    "timeseries_data.columns = timeseries_data.columns.droplevel(-1)\n",
    "timeseries_data = pd.merge(timeseries_data.reset_index(), one_hot.reset_index(), how='inner', left_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'],\n",
    "                           right_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])\n",
    "timeseries_data = timeseries_data.set_index(['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-sister",
   "metadata": {},
   "source": [
    "### Preprocessing of Y / outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cardiovascular-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hospital_mortality    0\n",
      "los                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First get the number of nan values per variable:\n",
    "print(outcomes.isna().sum())\n",
    "\n",
    "# We will replace them with zero:\n",
    "outcomes = outcomes.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-arnold",
   "metadata": {},
   "source": [
    "### Save all pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continuing-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and save the results:\n",
    "s = timeseries_data.columns.to_series()\n",
    "timeseries_data.columns = s + s.groupby(s).cumcount().astype(str).replace({'0':''})\n",
    "\n",
    "timeseries_data.to_hdf('vitals_hourly_data_preprocessed.h5', 'X')\n",
    "outcomes.to_hdf('vitals_hourly_data_preprocessed.h5', 'Y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
