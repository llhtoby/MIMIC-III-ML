{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of machine learning models to process Electronic Health Records â€“ Explainable Models\n",
    "\n",
    "### Preprocessing Notebook\n",
    "Lok Hang Toby Lee (2431180L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mc\n",
    "import colorsys\n",
    "import psycopg2\n",
    "import os\n",
    "import yaml\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#pg_ctl.exe restart -D \"E:\\PostgreSQL\\data\"\n",
    "\n",
    "# Configuration:\n",
    "sqluser = 'postgres'\n",
    "dbname = 'mimic'\n",
    "password='postgres'\n",
    "schema_name = 'public, mimic, mimiciii;'\n",
    "\n",
    "# Connect to MIMIC-III:\n",
    "con = psycopg2.connect(dbname=dbname, user=sqluser, password=password)\n",
    "cur = con.cursor()\n",
    "cur.execute('SET search_path to ' + schema_name)\n",
    "\n",
    "\n",
    "# SET YOUR PATH FOR RESOURCES FILE HERE\n",
    "resources_path = \"E:/MIMIC-III-ML/Workspace/resources/\"\n",
    "data_path = \"E:/MIMIC-III-ML/Workspace/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study cohort selection\n",
    "- Only first ICU admissions that took at least a day and less than 10 days\n",
    "- Adult patients only (age >= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the query:\n",
    "min_age = 15\n",
    "limit_population = 0 # if we want to run the query for a small number of patients (for debugging)\n",
    "if limit_population > 0:\n",
    "    limit = 'LIMIT ' + str(limit_population)\n",
    "else:\n",
    "    limit = ''\n",
    "    \n",
    "query = \"\"\"\n",
    "with patient_and_icustay_details as (\n",
    "    SELECT distinct\n",
    "        p.gender, p.dob, p.dod, s.*, a.admittime, a.dischtime, a.deathtime, a.ethnicity, a.diagnosis,\n",
    "        DENSE_RANK() OVER (PARTITION BY a.subject_id ORDER BY a.admittime) AS hospstay_seq,\n",
    "        DENSE_RANK() OVER (PARTITION BY s.hadm_id ORDER BY s.intime) AS icustay_seq,\n",
    "        DATE_PART('year', s.intime) - DATE_PART('year', p.dob) as admission_age,\n",
    "        DATE_PART('day', s.outtime - s.intime) as los_icu\n",
    "    FROM patients p \n",
    "        INNER JOIN icustays s ON p.subject_id = s.subject_id\n",
    "        INNER JOIN admissions a ON s.hadm_id = a.hadm_id \n",
    "    WHERE s.first_careunit NOT like 'NICU'\n",
    "        and s.hadm_id is not null and s.icustay_id is not null\n",
    "        and (s.outtime >= (s.intime + interval '12 hours'))\n",
    "        and (s.outtime <= (s.intime + interval '240 hours'))\n",
    "    ORDER BY s.subject_id \n",
    ")\n",
    "SELECT * \n",
    "FROM patient_and_icustay_details \n",
    "WHERE hospstay_seq = 1\n",
    "    and icustay_seq = 1\n",
    "    and admission_age >=  \"\"\" + str(min_age) + \"\"\"\n",
    "    and los_icu >= 0.5\n",
    "\"\"\" + str(limit)\n",
    "patients_data = pd.read_sql_query('SET search_path to ' + schema_name + query, con)\n",
    "\n",
    "# Save result:\n",
    "#patients_data.to_csv('static_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_keep = ('Capillary refill rate', 'Diastolic blood pressure', 'Fraction inspired oxygen', \n",
    "                     'Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total',\n",
    "                     'Glascow coma scale verbal response', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure',\n",
    "                     'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH')\n",
    "var_map = pd.read_csv(resources_path + 'itemid_to_variable_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_ids_to_keep = patients_data['icustay_id']\n",
    "icu_ids_to_keep = tuple(set([str(i) for i in icu_ids_to_keep]))\n",
    "subjects_to_keep = patients_data['subject_id']\n",
    "subjects_to_keep = tuple(set([str(i) for i in subjects_to_keep]))\n",
    "hadms_to_keep = patients_data['hadm_id']\n",
    "hadms_to_keep = tuple(set([str(i) for i in hadms_to_keep]))\n",
    "\n",
    "labitems_to_keep = []\n",
    "chartitems_to_keep = []\n",
    "for i in range(var_map.shape[0]):\n",
    "    if var_map['LEVEL2'][i] in variables_to_keep:\n",
    "        if var_map['LINKSTO'][i] == 'chartevents':\n",
    "            chartitems_to_keep.append(var_map['ITEMID'][i])\n",
    "        elif var_map['LINKSTO'][i] == 'labevents':\n",
    "            labitems_to_keep.append(var_map['ITEMID'][i])\n",
    "            \n",
    "all_to_keep = chartitems_to_keep + labitems_to_keep\n",
    "var_map = var_map[var_map.ITEMID.isin(all_to_keep)]\n",
    "chartitems_to_keep = tuple(set([str(i) for i in chartitems_to_keep]))\n",
    "labitems_to_keep = tuple(set([str(i) for i in labitems_to_keep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "duplicate-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "with patient_and_icustay_details as (\n",
    "    SELECT distinct\n",
    "        p.gender, p.dob, p.dod, s.*, a.admittime, a.dischtime, a.deathtime, a.ethnicity, a.diagnosis,\n",
    "        DENSE_RANK() OVER (PARTITION BY a.subject_id ORDER BY a.admittime) AS hospstay_seq,\n",
    "        DENSE_RANK() OVER (PARTITION BY s.hadm_id ORDER BY s.intime) AS icustay_seq,\n",
    "        DATE_PART('year', s.intime) - DATE_PART('year', p.dob) as admission_age,\n",
    "        DATE_PART('day', s.outtime - s.intime) as los_icu\n",
    "    FROM patients p \n",
    "        INNER JOIN icustays s ON p.subject_id = s.subject_id\n",
    "        INNER JOIN admissions a ON s.hadm_id = a.hadm_id \n",
    "    WHERE s.first_careunit NOT like 'NICU'\n",
    "        and s.hadm_id is not null and s.icustay_id is not null\n",
    "        and (s.outtime >= (s.intime + interval '12 hours'))\n",
    "        and (s.outtime <= (s.intime + interval '240 hours'))\n",
    "    ORDER BY s.subject_id \n",
    ")\n",
    "SELECT * \n",
    "FROM patient_and_icustay_details \n",
    "WHERE hospstay_seq = 1\n",
    "    and icustay_seq = 1\n",
    "    and admission_age >=  \"\"\" + str(min_age) + \"\"\"\n",
    "    and los_icu >= 0.5\n",
    "\"\"\" + str(limit)\n",
    "patients_data = pd.read_sql_query('SET search_path to ' + schema_name + query, con)\n",
    "\n",
    "# Save result:\n",
    "#patients_data.to_csv(data_path + 'static_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT c.subject_id, i.hadm_id, c.icustay_id, c.charttime, c.itemid, c.value, c.valueuom\n",
    "FROM icustays i\n",
    "INNER JOIN chartevents c ON i.icustay_id = c.icustay_id\n",
    "where c.icustay_id in \"\"\" + str(icu_ids_to_keep) + \"\"\"\n",
    "  and c.itemid in \"\"\" + str(chartitems_to_keep) + \"\"\"\n",
    "  and c.charttime between intime and outtime\n",
    "  and c.error is distinct from 1\n",
    "  and c.valuenum is not null\n",
    "UNION ALL\n",
    "SELECT distinct i.subject_id, i.hadm_id, i.icustay_id, l.charttime, l.itemid, l.value, l.valueuom\n",
    "FROM icustays i\n",
    "INNER JOIN labevents l ON i.hadm_id = l.hadm_id\n",
    "where i.icustay_id in \"\"\" + str(icu_ids_to_keep) + \"\"\"\n",
    "  and l.itemid in \"\"\" + str(labitems_to_keep) + \"\"\"\n",
    "  and l.charttime between (intime - interval '6' hour) and outtime\n",
    "  and l.valuenum > 0 -- lab values cannot be 0 and cannot be negative\n",
    "\"\"\"\n",
    "events_data = pd.read_sql_query('SET search_path to ' + schema_name + query, con)\n",
    "events_data.to_csv(data_path + 'events_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>value</th>\n",
       "      <th>valueuom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220179</td>\n",
       "      <td>132</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220180</td>\n",
       "      <td>78</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220181</td>\n",
       "      <td>89</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220210</td>\n",
       "      <td>17</td>\n",
       "      <td>insp/min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>293876</td>\n",
       "      <td>2168-07-11 15:00:00</td>\n",
       "      <td>220045</td>\n",
       "      <td>75</td>\n",
       "      <td>bpm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  hadm_id  icustay_id           charttime  itemid value  valueuom\n",
       "0         266   186251      293876 2168-07-11 14:00:00  220179   132      mmHg\n",
       "1         266   186251      293876 2168-07-11 14:00:00  220180    78      mmHg\n",
       "2         266   186251      293876 2168-07-11 14:00:00  220181    89      mmHg\n",
       "3         266   186251      293876 2168-07-11 14:00:00  220210    17  insp/min\n",
       "4         266   186251      293876 2168-07-11 15:00:00  220045    75       bpm"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemids = tuple(set(events_data.itemid.astype(str)))\n",
    "events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_d_items = \\\n",
    "        \"\"\"\n",
    "        SELECT itemid, label, dbsource, linksto, category, unitname\n",
    "        FROM d_items\n",
    "        WHERE itemid in \"\"\" + str(itemids)\n",
    "d_output = pd.read_sql_query('SET search_path to ' + schema_name + query_d_items, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the text from the categorical (Glasgow coma scale) variables so we can make them numeric:\n",
    "replacement_dictionary = {'4 Spontaneously': '4', '3 To speech': '3', '2 To pain': '2', '1 No Response': '1',\n",
    "                         '5 Oriented': '5', '1.0 ET/Trach': '1', '4 Confused': '4', '2 Incomp sounds': '2', \n",
    "                         '3 Inapprop words': '3', 'Spontaneously': '4', 'To Speech': '3', 'None': '1', 'To Pain': '2',\n",
    "                         '6 Obeys Commands': '6', '5 Localizes Pain': '5', '4 Flex-withdraws': '4', '2 Abnorm extensn': '2',\n",
    "                         '3 Abnorm flexion': '3', 'No Response-ETT': '1', 'Oriented': '5', 'Confused': '4', \n",
    "                         'No Response': '1', 'Incomprehensible sounds': '2', 'Inappropriate Words': '3', \n",
    "                         'Obeys Commands': '6', 'No response': '1', 'Localizes Pain': '5', 'Flex-withdraws': '4',\n",
    "                         'Abnormal extension': '2', 'Abnormal flexion': '3', 'Abnormal Flexion': '3', \n",
    "                          'Abnormal Extension': '2'}\n",
    "for key, value in replacement_dictionary.items():\n",
    "    events_data['value'] = events_data['value'].replace(key, value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data types and set indices:\n",
    "events_data['value'] = pd.to_numeric(events_data['value']) #, 'coerce')\n",
    "events_data = events_data.astype({k: int for k in ['subject_id', 'hadm_id', 'icustay_id']})\n",
    "patients_data = patients_data.reset_index().set_index('icustay_id')\n",
    "var_map = var_map[['LEVEL2', 'ITEMID', 'LEVEL1']].rename(\n",
    "    {'LEVEL2': 'LEVEL2', 'LEVEL1': 'LEVEL1', 'ITEMID': 'itemid'}, axis=1).set_index('itemid')\n",
    "\n",
    "# Change to hourly data:\n",
    "to_hours = lambda x: max(0, x.days*24 + x.seconds // 3600)\n",
    "events_data = events_data.set_index('icustay_id').join(patients_data[['intime']])\n",
    "events_data['hours_in'] = (events_data['charttime'] - events_data['intime']).apply(to_hours)\n",
    "events_data = events_data.drop(columns=['charttime', 'intime']) \n",
    "\n",
    "# Join with d_output query and group variables:\n",
    "events_data = events_data.set_index('itemid', append=True)\n",
    "events_data = events_data.join(var_map)\n",
    "d_output = d_output.set_index('itemid')\n",
    "events_data = events_data.join(d_output) \n",
    "events_data = events_data.set_index(['label', 'LEVEL1', 'LEVEL2'], append=True)\n",
    "patients_data['max_hours'] = (patients_data['outtime'] - patients_data['intime']).apply(to_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2530: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->['gender', 'dbsource', 'first_careunit', 'last_careunit', 'ethnicity', 'diagnosis']]\n",
      "\n",
      "  pytables.to_hdf(path_or_buf, key, self, **kwargs)\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2530: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->['valueuom', 'dbsource', 'linksto', 'category', 'unitname']]\n",
      "\n",
      "  pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Save results:\n",
    "patients_data.to_hdf(data_path + 'vitals_hourly_data.h5', 'patients_data')\n",
    "events_data.to_hdf(data_path + 'vitals_hourly_data.h5', 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-cleanup",
   "metadata": {},
   "source": [
    "### Extract length of stay and in-hospital mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.DataFrame(index=patients_data.index)\n",
    "# In hospital mortality: patient has died after the admittime to hospital and before the outtime:\n",
    "mortality = patients_data.dod.notnull() & ((patients_data.admittime <= patients_data.dod) & (patients_data.outtime >= patients_data.dod))\n",
    "mortality = mortality | (patients_data.deathtime.notnull() & ((patients_data.admittime <= patients_data.deathtime) & \n",
    "                                                             (patients_data.dischtime >= patients_data.deathtime)))\n",
    "outcomes['in_hospital_mortality'] = mortality.astype(int)\n",
    "\n",
    "# Length of stay (in hours):\n",
    "outcomes['los'] = patients_data['los'] * 24.0\n",
    "outcomes.to_hdf(data_path + 'vitals_hourly_data.h5', 'Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to csv\n",
    "events_data.to_csv(data_path + 'events_data.csv')\n",
    "patients_data.to_csv(data_path + 'patients_data.csv')\n",
    "outcomes.to_csv(data_path + 'outcomes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_data (X):  (20567026, 14)\n",
      "patients_data:  (30063, 25)\n",
      "outcomes (Y):  (30063, 2)\n",
      "Categorical:  ['Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', 'Glascow coma scale verbal response']\n",
      "Continuous:  ['Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n"
     ]
    }
   ],
   "source": [
    "events_data = pd.read_hdf(data_path + 'vitals_hourly_data.h5', 'X')\n",
    "events_data = events_data.reset_index()\n",
    "print('events_data (X): ', events_data.shape)\n",
    "\n",
    "patients_data = pd.read_hdf(data_path + 'vitals_hourly_data.h5', 'patients_data')\n",
    "print('patients_data: ', patients_data.shape)\n",
    "\n",
    "outcomes = pd.read_hdf(data_path + 'vitals_hourly_data.h5', 'Y')\n",
    "print('outcomes (Y): ', outcomes.shape)\n",
    "\n",
    "# Load the config file that contains information about continuous/categorical variables:\n",
    "config = json.load(open(resources_path + 'discretizer_config.json', 'r'))\n",
    "is_categorical = config['is_categorical_channel']\n",
    "\n",
    "# Get categorical variables:\n",
    "categorical_var = []\n",
    "continuous_var = []\n",
    "for key, value in is_categorical.items():\n",
    "    if value:\n",
    "        categorical_var.append(key)\n",
    "    else:\n",
    "        continuous_var.append(key)\n",
    "print('Categorical: ', categorical_var[1:])\n",
    "print('Continuous: ', continuous_var)\n",
    "categorical_var = categorical_var[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map variables to the same metric:\n",
    "UNIT_CONVERSIONS = [\n",
    "    ('weight',                   'oz',  None,             lambda x: x/16.*0.45359237),\n",
    "    ('weight',                   'lbs', None,             lambda x: x*0.45359237),\n",
    "    ('fraction inspired oxygen', None,  lambda x: x > 1,  lambda x: x/100.),\n",
    "    ('oxygen saturation',        None,  lambda x: x <= 1, lambda x: x*100.),\n",
    "    ('temperature',              'f',   lambda x: x > 79, lambda x: (x - 32) * 5./9),\n",
    "    ('height',                   'in',  None,             lambda x: x*2.54),\n",
    "]\n",
    "\n",
    "variable_names = events_data['LEVEL1'].str\n",
    "variable_units = events_data['valueuom'].str\n",
    "for name, unit, check, convert_function in UNIT_CONVERSIONS:\n",
    "    indices_variable = variable_names.contains(name, case=False, na=False)\n",
    "    needs_conversion_filter_indices = indices_variable & False\n",
    "    if unit is not None:\n",
    "        needs_conversion_filter_indices |= variable_names.contains(unit, case=False, na=False) | variable_units.contains(unit, case=False, na=False)\n",
    "    if check is not None:\n",
    "        needs_conversion_filter_indices |= check(events_data['value'])\n",
    "    idx = indices_variable & needs_conversion_filter_indices\n",
    "    events_data.loc[idx, 'value'] = convert_function(events_data['value'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and remove outliers. For this, they use two different outlier ranges: \n",
    "# 1) for each variable, they have an upper and lower threshold for detecting unusable outliers. \n",
    "#    If the outlier falls outside of these threshold, it is treated as missing. \n",
    "# 2) they also have a physiologically valid range of measurements. If the non-outlier falls outside this range, \n",
    "     # it is replaced with the nearest valid value.\n",
    "\n",
    "variable_ranges = pd.read_csv(resources_path + 'variable_ranges.csv', index_col = None)\n",
    "variable_ranges['LEVEL2'] = variable_ranges['LEVEL2'].str.lower()\n",
    "variable_ranges = variable_ranges.set_index('LEVEL2')\n",
    "\n",
    "variables_all = events_data['LEVEL2']\n",
    "non_null_variables = ~events_data.value.isnull()\n",
    "variables = set(variables_all)\n",
    "range_names = set(variable_ranges.index.values)\n",
    "range_names = [i.lower() for i in range_names]\n",
    "\n",
    "for var_name in variables:\n",
    "    var_name_lower = var_name.lower()\n",
    "    \n",
    "    if var_name_lower in range_names:\n",
    "        out_low, out_high, val_low, val_high = [\n",
    "            variable_ranges.loc[var_name_lower, x] for x in ('OUTLIER LOW', 'OUTLIER HIGH', 'VALID LOW', 'VALID HIGH')\n",
    "        ]\n",
    "        \n",
    "        # First find the indices of the variables that we need to check for outliers:\n",
    "        indices_variable = non_null_variables & (variables_all == var_name)\n",
    "        \n",
    "        # Check for low outliers and if they are not extreme, replace them with the imputation value:\n",
    "        outlier_low_indices = (events_data.value < out_low)\n",
    "        low_not_outliers = ~outlier_low_indices & (events_data.value < val_low)\n",
    "        valid_low_indices = indices_variable & low_not_outliers\n",
    "        events_data.loc[valid_low_indices, 'value'] = val_low\n",
    "        \n",
    "        # Check for high outliers and if they are not extreme, replace them with the imputation value:\n",
    "        outlier_high_indices = (events_data.value > out_high)\n",
    "        high_not_outliers = ~outlier_high_indices & (events_data.value > val_high)\n",
    "        valid_high_indices = indices_variable & high_not_outliers\n",
    "        events_data.loc[valid_high_indices, 'value'] = val_high\n",
    "        \n",
    "        # Treat values that are outside the outlier boundaries as missing:\n",
    "        outlier_indices = indices_variable & (outlier_low_indices | outlier_high_indices)\n",
    "        events_data.loc[outlier_indices, 'value'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data\n",
    "We want to have a column for every variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data = events_data.set_index(['icustay_id', 'itemid', 'label', 'LEVEL1', 'LEVEL2'])\n",
    "events_data = events_data.groupby(['icustay_id', 'subject_id', 'hadm_id', 'LEVEL2', 'hours_in'])\n",
    "events_data = events_data.agg(['mean', 'std', 'count'])\n",
    "events_data.columns = events_data.columns.droplevel(0)\n",
    "events_data.columns.names = ['Aggregation Function']\n",
    "events_data = events_data.unstack(level = 'LEVEL2')\n",
    "events_data.columns = events_data.columns.reorder_levels(order=['LEVEL2', 'Aggregation Function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a row for every hour:\n",
    "missing_hours_fill = pd.DataFrame([[i, x] for i, y in patients_data['max_hours'].iteritems() for x in range(y+1)],\n",
    "                                 columns=[patients_data.index.names[0], 'hours_in'])\n",
    "missing_hours_fill['tmp'] = np.NaN\n",
    "\n",
    "fill_df = patients_data.reset_index()[['subject_id', 'hadm_id', 'icustay_id']].join(\n",
    "     missing_hours_fill.set_index('icustay_id'), on='icustay_id')\n",
    "fill_df.set_index(['icustay_id', 'subject_id', 'hadm_id', 'hours_in'], inplace=True)\n",
    "\n",
    "events_data = events_data.reindex(fill_df.index)\n",
    "events_data = events_data.sort_index(axis = 0).sort_index(axis = 1)\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "events_data.loc[:, idx[:, 'count']] = events_data.loc[:, idx[:, 'count']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this version of the data as a .csv file, so we can apply different imputation methods in another notebook:\n",
    "idx = pd.IndexSlice\n",
    "timeseries_data = events_data.loc[:, idx[:, 'mean']]\n",
    "timeseries_data = timeseries_data.droplevel('Aggregation Function', axis = 1) \n",
    "timeseries_data = timeseries_data.reset_index() \n",
    "timeseries_data.to_csv(data_path + 'mimic_timeseries_data_not_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "idx = pd.IndexSlice\n",
    "timeseries_data = events_data.loc[:, idx[:, ['mean', 'count']]]\n",
    "\n",
    "# Get the mean across hours for each variable and each patient:\n",
    "icustay_means = timeseries_data.loc[:, idx[:, 'mean']].groupby(['subject_id', 'hadm_id', 'icustay_id']).mean()\n",
    "\n",
    "# Get the global mean for each variable:\n",
    "global_means = timeseries_data.loc[:, idx[:, 'mean']].mean(axis = 0)\n",
    "\n",
    "# Forward fill the nan time series, or otherwise fill in the patient's mean or global mean:\n",
    "timeseries_data.loc[:, idx[:, 'mean']] = timeseries_data.loc[:, idx[:, 'mean']].groupby(\n",
    "    ['subject_id', 'hadm_id', 'icustay_id']).fillna(method='ffill').groupby(\n",
    "    ['subject_id', 'hadm_id', 'icustay_id']).fillna(icustay_means).fillna(global_means)\n",
    "\n",
    "# Create a mask that indicates if the variable is present:\n",
    "timeseries_data.loc[:, idx[:, 'count']] = (events_data.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "timeseries_data.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "# Add a variable that indicates the time since the last measurement to the dataframe:\n",
    "is_absent = (1 - timeseries_data.loc[:, idx[:, 'mask']])\n",
    "hours_of_absence = is_absent.cumsum()\n",
    "time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "timeseries_data = pd.concat((timeseries_data, time_since_measured), axis = 1)\n",
    "timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].fillna(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minmax standardization:\n",
    "def minmax(x):\n",
    "    mins = x.min()\n",
    "    maxes = x.max()\n",
    "    x_std = (x - mins) / (maxes - mins)\n",
    "    return x_std\n",
    "\n",
    "def std_time_since_measurement(x):\n",
    "    idx = pd.IndexSlice\n",
    "    x = np.where(x==100, 0, x)\n",
    "    means = x.mean()\n",
    "    stds = x.std() + 0.0001\n",
    "    x_std = (x - means)/stds\n",
    "    return x_std\n",
    "\n",
    "timeseries_data.loc[:, idx[continuous_var, 'mean']] = timeseries_data.loc[:, idx[continuous_var, 'mean']].apply(lambda x: minmax(x))\n",
    "timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3946: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  new_axis = axis.drop(labels, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# First we need to round the categorical variables to the nearest category:\n",
    "categorical_data = timeseries_data.loc[:, idx[categorical_var, 'mean']].copy(deep=True)\n",
    "categorical_data = categorical_data.round()\n",
    "one_hot = pd.get_dummies(categorical_data, columns=categorical_var)\n",
    "\n",
    "# Clean up the columns that we do not need and add the dummy encodings:\n",
    "for c in categorical_var:\n",
    "    if c in timeseries_data.columns:\n",
    "        timeseries_data.drop(c, axis = 1, inplace=True)\n",
    "timeseries_data.columns = timeseries_data.columns.droplevel(-1)\n",
    "timeseries_data = pd.merge(timeseries_data.reset_index(), one_hot.reset_index(), how='inner', left_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'],\n",
    "                           right_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])\n",
    "timeseries_data = timeseries_data.set_index(['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Y / outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hospital_mortality    0\n",
      "los                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First get the number of nan values per variable:\n",
    "print(outcomes.isna().sum())\n",
    "\n",
    "# We will replace them with zero:\n",
    "outcomes = outcomes.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and save the results:\n",
    "s = timeseries_data.columns.to_series()\n",
    "timeseries_data.columns = s + s.groupby(s).cumcount().astype(str).replace({'0':''})\n",
    "\n",
    "timeseries_data.to_hdf(data_path + 'vitals_hourly_data_preprocessed.h5', 'X')\n",
    "outcomes.to_hdf(data_path + 'vitals_hourly_data_preprocessed.h5', 'Y')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e48d23369a553edc96f1373b6255b5687d82d68cd08867622b2500e338930542"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
